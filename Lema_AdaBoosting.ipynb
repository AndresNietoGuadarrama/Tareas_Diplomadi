{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equivalencia entre el método Ada-Boosting y Forward Stageway Additive Model \n",
    "## Andrés Nieto Guadarrama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lema:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encontrar la clasificación óptima mediante el método de $\\textit{Ada Boosting}$ equivale a implementar el modelo aditivo con la función de costo\n",
    "$$$$\n",
    "$$L\\left(y,f(x)\\right)\\,=\\,e^{-yf(x)}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demostración"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observar que, para cada paso $m$, el método del modelo aditivo busca al clasificador $G_{m}$y al parámetro $\\beta_{m}$ tales que:\n",
    "$$(\\beta_{m},G_{m})\\,=\\,\\min_{\\beta,G} \\left \\{ \\sum_{i=1}^{N} e^{-y_{i}(f_{m-1}(x_{i})+\\beta G(x_{i}))} \\right \\}\\,=\\,\\min_{\\beta,G} \\left \\{ \\sum_{i=1}^{N} e^{-y_{i}f_{m-1}(x_{i})}e^{-y_{i}\\beta G(x_{i})} \\right \\}\n",
    "=\\,\\min_{\\beta,G} \\left \\{ \\sum_{i=1}^{N} w_{i}^{(m)}e^{-y_{i}\\beta G(x_{i})} \\right \\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "con $w_{i}^{(m)}\\,=\\,e^{-y_{i}f_{m-1}(x_{i})}\\,>\\,0$ independiente de $G$ y $\\beta$. Más aún, partiendo la suma anterior tenemos que"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sum_{i=1}^{N} w_{i}^{(m)}e^{-y_{i}\\beta G(x_{i})}\\,=\\,\\sum_{\\{i:y_{i}=G(x_{i})\\}}w_{i}^{(m)}e^{-\\beta}+\\sum_{\\{i:y_{i} \\neq G(x_{i})\\}}w_{i}^{(m)}e^{\\beta}\\,=\\,\\sum_{i=1}^{N}w_{i}^{(m)}e^{-\\beta}-\\sum_{\\{i:y_{i} \\neq G(x_{i})\\}}w_{i}^{(m)}e^{-\\beta}+\\sum_{\\{i:y_{i} \\neq G(x_{i})\\}}w_{i}^{(m)}e^{\\beta}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "con lo cual,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sum_{i=1}^{N} w_{i}^{(m)}e^{-y_{i}\\beta G(x_{i})}\\,=\\,\\sum_{i=1}^{N}w_{i}^{(m)}e^{-\\beta}+(e^\\beta-e^{-\\beta})\\sum_{\\{i:y_{i} \\neq G(x_{i})\\}}w_{i}^{(m)}\\,=\\,\\sum_{i=1}^{N}w_{i}^{(m)}e^{-\\beta}+(e^\\beta-e^{-\\beta})\\sum_{i=1}^N w_{i}^{(m)}\\mathbb{1}_{\\{y_{i} \\neq G(x_{i})\\}}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que el único elemento de la suma que se ve afectado por la función clasificadora es $\\sum_{i=1}^N w_{i}^{(m)}\\mathbb{1}_{\\{y_{i} \\neq G(x_{i})\\}}$, tenemos que para cada paso $m$\n",
    "$$G_{m}\\,=\\,\\min_{G}\\left \\{ \\sum_{i=1}^N w_{i}^{(m)}\\mathbb{1}_{\\{y_{i} \\neq G(x_{i})\\}} \\right \\}_{.}$$ Notemos que a partir de éstos cálculos ya se consiguió el clasificador $G_{m}$ y los pesos $w_{i}^{(m)}$ requeridos como primer paso en el algoritmo $\\textit{Ada Boosting M1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por otro lado, sustituyendo a $G$ por $G_m$ en la función objetivo, podemos notar que las sumas $\\sum_{i=1}^{N}w_{i}^{(m)}$ y $ \\sum_{i=1}^N w_{i}^{(m)}\\mathbb{1}_{\\{y_{i} \\neq G(x_{i})\\}} $ no dependen de $\\beta$, por lo que $\\beta_{m}$ se encuentra minimizando la función en $\\mathbb{R}$: \n",
    "$$g(\\beta)\\,=\\,Ae^{-\\beta}+B(e^\\beta-e^{-\\beta}),=\\,(A-B)e^{-\\beta}+Be^\\beta$$\n",
    "con $A\\,=\\,\\sum_{i=1}^{N}w_{i}^{(m)}$ y $B\\,=\\,\\sum_{i=1}^N w_{i}^{(m)}\\mathbb{1}_{\\{y_{i} \\neq G_{m}(x_{i})\\}}$\n",
    "$$$$\n",
    "Por condiciones de primer orden, $\\beta_m$ debe cumplir que:\n",
    "$$0\\,=\\,g'(\\beta_m)\\,=\\,(B-A)e^{-\\beta_m}+Be^\\beta_m\\,=\\,\\frac{1}{e^{\\beta_m}}((B-A)+Be^{2\\beta_m}),$$\n",
    "$$$$\n",
    "esto es, \n",
    "$$\\beta_{m}\\,=\\,\\frac{1}{2}ln\\left (\\frac{A-B}{B} \\right )=\\,\\frac{1}{2}ln\\left (\\frac{\\sum_{i=1}^N w_{i}^{(m)}\\mathbb{1}_{\\{y_{i} = G_{m}(x_{i})\\}}}{\\sum_{i=1}^N w_{i}^{(m)}\\mathbb{1}_{\\{y_{i} \\neq G_{m}(x_{i})\\}}} \\right )\\,=\\,\\frac{1}{2}ln\\left (\\frac{1-err_m}{err_m}\\right)$$\n",
    "$$$$\n",
    "donde $err_m=\\frac{B}{A}$ es el error de predicción de la función clasificadora $G_m$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez encontradas $\\beta_m$ y $G_m$, el siguiente paso del modelo aditivo nos lleva a actualizar las funciones $f_m$ de manera que:\n",
    "$$f_m\\,=\\,f_{m-1} + G_m\\beta_m.$$\n",
    "Sin embargo, por la forma en que definimos los pesos $w_i^{m}$, tenemos que para el siguiente paso: \n",
    "$$$$\n",
    "$$w_{i}^{m+1}\\,=\\,e^{-y_{i}f_{m}(x_{i})}\\,=\\,e^{-y_{i}f_{m-1}(x_{i})} e^{-y_{i}G_m(x_{i})\\beta_m}\\,=\\,w_{i}^{m}e^{-y_{i}G_m(x_{i})\\beta_m}$$\n",
    "$$$$\n",
    "Más aún, dado que $-y_{i}G_m(x_{i}) \\,=\\, -1$ si $y_{i}\\,=\\,G_m(x_{i})$ y $-y_{i}G_m(x_{i}) \\,=\\, 1$ si $y_{i}\\, \\neq \\,G_m(x_{i})$, podemos escribir a $-y_{i}G_m(x_{i})$ como\n",
    "$$$$\n",
    "$$-y_{i}G_m(x_{i})\\,=\\,2\\mathbb{1}_{\\{y_{i} \\neq G_{m}(x_{i})\\}} -1$$\n",
    "Con lo cual, \n",
    "$$$$\n",
    "$$w_{i}^{m+1}\\,=\\,w_{i}^{m}e^{2\\beta_m\\mathbb{1}_{\\{y_{i} \\neq G_{m}(x_{i})\\}}}e^{-\\beta_m}$$\n",
    "$$$$\n",
    "Finalmente, observar que el factor $e^{-\\beta_m}$ afecta a todas las $w_{i}^{m+1}$ para toda $i\\,=\\,1,...,N$, por lo que puede omitirse. Por lo tanto, se obtuvieron los mismos valores para $\\beta$, $G$ y $w_i^m$ que si se hubiera seguido el algoritmo de $\\textit{AdaBoosting M1}{\\blacksquare}$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
